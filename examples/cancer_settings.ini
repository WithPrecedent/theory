[general]
verbose = True
seed = 43
conserve_memory = True
parallelize = False
gpu = False

[project]
project_workers = analyst, critic

[files]
source_format = csv
interim_format = csv
final_format = csv
analysis_format = csv
file_encoding = windows-1252
float_format = %.4f
test_data = True
test_chunk = 500
random_test_chunk = True
boolean_out = True
naming_classes = model, cleave
export_results = True

[wrangler]
wrangler_steps = none

[analyst]
analyst_steps = categorize, scale, split, encode, mix, cleave, sample, reduce, model
data_to_use = train_test
model_type = classify
label = target
calculate_hyperparameters = False
export_all_recipes = True
fill_techniques = none
categorize_techniques = none
scale_techniques = minmax
split_techniques = stratified
encode_techniques = target, woe
mix_techniques = none
cleave_techniques = none
sample_techniques = none
reduce_techniques = none
model_techniques = xgboost
search_method = random

[explorer]
explorer_steps = summary
summary_techniques = default
test_techniques = none

[critic]
critic_steps = explain, predict, report
critic_techniques = shap, sklearn
data_to_review = test
join_predictions = True

[artist]
artist_steps = illustrator, painter
illustrator_techniques = default
painter_techniques = default
animator_techniques = default
data_to_plot = test
comparison_plots = False

[styler_parameters]
plot_style = fivethirtyeight
plot_font = Franklin Gothic Book
seaborn_style = darkgrid
seaborn_context = paper
seaborn_palette = dark
interactions_display = 10
features_display = 20
summary_display = 20
dependency_plots = cleave, top_features
shap_plot_type = dot

[scaler_parameters]
copy = False
encode = ordinal
strategy = uniform
n_bins = 5

[splitter_parameters]
test_size = 0.33
val_size = 0
n_splits = 5
shuffle = False

[encoder_parameters]

[mixer_parameters]

[cleaver_parameters]
include_all = True

[sampler_parameters]
sampling_strategy = auto

[reducer_parameters]
n_features_to_select = 10
step = 1
score_func = f_classif
alpha = 0.05
threshold = mean

[search_parameters]
n_iter = 50
scoring = roc_auc, f1, neg_log_loss
cv = 5
refit = True

[random_forest_parameters]
n_estimators = 20, 1000
max_depth = 5, 30
max_features = 10, 50
max_leaf_nodes = 5, 10
bootstrap = True
oob_score = True
verbose = 0

[xgboost_parameters]
booster = gbtree
objective = binary:logistic
eval_metric = aucpr
silent = True
n_estimators = 50
max_depth = 5
learning_rate = 0.001
subsample = 0.3
colsample_bytree = 0.3
colsample_bylevel = 0.3
min_child_weight = 0.7
gamma = 0.0
alpha = 0.0

[tensorflow_parameters]

[baseline_classifier_parameters]
strategy = most_frequent
